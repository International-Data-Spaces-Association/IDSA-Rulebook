#### Access policies

A best practice of access security is for an IT system to show users
only what they need to know - to minimize the potential attack surface.
The same is true for data contract offers (DCO) in a data space:
Participants should only see the DCOs for which they are authorized to
request a contract negotiation. This does not imply that the participant
already has authorization for the data but only that a participant is
allowed to see that the data exists. The permission to access is part of
the data contract negotiation. Any catalog must implement
attribute-based access control (ABAC) through access policies.

The most common access filter is that a participant proves membership to
see which assets are in a data space. Filters can also be applied that
make data assets accessible only to specific participant groups. For
example, a participant who has a VC as a data space member, but also has
an additional VC which attests that the participant is an auditor, could
provide this participant access to audit log files or streams which are
being shared as DCOs, but should not be visible to participants without
the special auditor credentials.

In case a participant wants to make a DCO visible to other entities that
are not participating in the data space and are merely using the
technical mechanisms of the data space or have been directly informed
about the existence of those DCOs, they could have an access policy
which is simply a no-op, or allow-all policy.

Access policies can also be used as filters to control visibility/access
to DCOs. For example, time-based policies can be used to control when
DCOs can be negotiated, location-based policies can limit the audience
to participants from a specific geographic region.

### Data sharing

Once a participant has joined a data space and discovered available data
contract offers, the mechanism of data sharing is initiated. Data
sharing is the core activity to enable further data processing and value
generation by using the data.

Data sharing is a very broad term in this context. It ranges from a
one-time transfer of a file, access to an API, registering for an
eventing service, subscribing to a data stream, also including data
sharing methods where the data remains at the source and algorithms and
processing code are copied to the data location for in-place processing.
Data Sharing does not require a physical move of the data asset,
although this will be frequently the case.

However, before data can be shared, a data contract offer needs to be
negotiated to reach a data contract agreement (DCA) which specifies all
policies and details of the data sharing process.

#### Contract negotiation

A contract negotiation (CN) serves the purpose of reaching an agreement
to share a data asset between two participants of the data space. During
the CN policies of the DCO are evaluated against the attributes of the
requesting participant, and VCs are verified with their issuers. Note
that while any trust anchor is an issuer of VCs that can be used to
evaluate policies, there might be additional external issuers that need
to be validated (e.g., government agencies, regulators, industry
associations)

It is important to note that the CN does not automatically lead to an
immediate data or algorithm transfer. The result of a CN is a data
contract agreement, which then can be executed at a later point in time.

Imagine a scenario where multiple roles are involved in the process of
data sharing in a large enterprise. The person negotiating the DCA might
not be the same one who is responsible for sharing the data. Or there
might be data assets that can't be immediately shared after the
agreement is reached (e.g., an event notification that can only be
consumed until the event in questions has occurred).

![Data sharing contract negotiation](media/media/image14.png)

**Data sharing execution**

When it is time to share the data, it might be necessary to re-validate
the policies of the data contract agreement as significant time might
have passed since the contract negotiation. The decision whether to
revisit all policies might depend on each party's business rules. If
data needs to be highly protected or requires specific regulatory
processes for handling it, it is advisable to conduct an additional
review.

To exercise a data contract agreement (which could also be code to
process data), data needs to be moved from one participant to another.
This can be done either by a push model in which the participant with
the data asset pushes the data to the other participant or by a pull
model, in which the data asset is made available to the consuming
participant via a link.

The data transfer technology depends on the type of data asset, trust
level, availability of technical protocols, infrastructure environment,
and other factors. All data transfer technologies must be able to be
orchestrated. Orchestration at this level means having technical control
over the data sharing process, allowing the connector to start and stop
the transfer, as well as having the necessary technical capabilities to
monitor the progress of the transfer and to receive information about
compliance with usage policies.

The transfer itself needs to ensure security, performance, and
manageability. For example, a data stream can be provided from multiple
data centers to enable a highly available data sharing architecture.

When data is not moved but a "code to data" approach is selected, the
push and pull behavior is reversed: The consumer participant provides a
data asset containing code (source code, compiled library, signed
container) to the participant providing the data. This can be
implemented like any other data asset transfer with a push or pull
mechanism.

Data sharing must accommodate a wide range of scenarios. From a simple
file transfer between two storage providers, to API access for streaming
or eventing, to quite complex implementations with secure execution
environments through confidential compute enclaves, environment
attestations, signed code, custom encryption algorithms, and more. Which
solution is right depends on the protection needs of the data and the
trust level between the participants.

The transfer technology can be specified as a policy in the data
contract agreement, or it can be implicitly inferred by the type of data
asset being shared. A participant who wants to ensure that data never
leaves an environment where full control over its usage is guaranteed
can enforce the selection of the transfer technology and storage and
processing infrastructure by setting policies in the contract and
monitoring compliance.

### Observability

In data spaces with highly regulated data, it is necessary to make the
data sharing process observable. This can be done for legal reasons to
prove that data has been processed only by authorized entities, or for
business reasons to provide a marketplace and billing function through a
trusted third party.

Depending on the architecture of the data space, multiple solutions are
possible. For a centralized architecture a central observer (sometimes
called clearing house, auditor or monitoring agent) can be implemented.
But this design has two shortcomings when implementing large-scale data
spaces: It presents an additional vulnerability that could affect the
sharing of mission critical data. And a central observer has data on all
DCAs which represents potentially valuable knowledge about the
participants. This can be exploited for financial gain, making it a
target for bad actors.

To address these risks, having at least a federated model of observers
is recommended to distribute the information, load, and potential for
error. To go a step further, a decentralized architecture can minimize
the risks associated with a centralized or federated observer model.

In a decentralized observer architecture, every participant keeps the
information about the agreed DCAs and their execution in their own
environment. Meaning that there are at least two copies of corresponding
logging information in the data space. The two copies can always be
identified through a correlation ID linking them. The observer then
matches the corresponding logging information and reports any
irregularities to the parties participating in the DCA (or to the
respective regulator if required).

A third party participant in the data space can have an additional VC
which qualifies them as a trusted observer, such as an industry auditor,
rooted in a governmental trust anchor for auditors.

To audit the contracts of a participant, the auditor would simply
request the log data which could then be published as data contract
offers with an access policy which restricts access to the auditor. To
verify the validity of those log entries, digital signing mechanism can
be used or the corresponding log data from other participants can be
requested (and again published as data contract offers). This would
limit access to sensitive observation data to observers that are
participants of the data space, have special credentials which qualify
them as trusted auditors and are bound to the policies of those
contracts due to the contracts on the collected log data. Observer
actions are automatically logged by the system and can be tracked and
monitored. This would enable a trust relationship in which auditors can
be audited by participants.

To simplify the observability of a data space, the DSGA can mandate that
participants make their audit data available as events or streams per
default. Then trusted auditors would not need to request publication but
could simply negotiate the relevant contracts, which are only accessible
to participants with valid auditing and monitoring credentials.

Following the same pattern, additional optional functional roles can be
implemented: a payment clearance service, notary services, regulatory
reporting, and the like.

### Vocabularies

Vocabularies are used to ensure that everyone means the same thing when
using a specific term. There are multiple vocabularies that are needed
in a data space, but two are particularly important:

- Semantic models for policies

- Semantic models of the shared data assets

So far, this document mostly described how a data space works, what
contracts are, what types of policies exist, and how to negotiate a
contract. The vocabularies describe the content of these elements.

The first category is the vocabulary of policies, which can exist on
multiple levels:

- Semantic model for policies for membership rules\
    For example, if a data space wants to restrict membership to
    companies with a HQ in certain countries. It must be clear what the
    policy is called and what values are allowed.

- Policies that each member of the data space must understand to
    interact with other participants. For example, policies that specify
    which industry vocabularies must be understood, and access policies.

- A participant can publish additional information on semantic models
    relevant for the interaction with this participant. This could be
    special access policies under which this participant publishes
    additional contracts. It could be an access policy that specifies
    access for direct suppliers of this participant.

- Data contract

- Semantic model which needs to be understood for a specific contract
    (e.g., special usage policy for a single contract)

The vocabularies for each level can be easily referenced by the metadata
publishing mechanism at the respective level. A data space can reference
the required policy vocabulary through its self-description. A
participant can also leverage its self-description to publish additional
vocabulary requirements. And at the data contract level, this
information can be easily stored in the metadata associated with the
contract at the catalog level.

For mandatory vocabularies a policy referencing them can be easily
established if such a policy model has been agreed upon.

Semantic models for data assets work on the same principle with the main
difference that they do not describe functionality of the data space
itself, but the meaning of the data being shared. If this data needs to
be understood to properly handle usage policies (e.g., if usage policies
are based on the meaning of data) it becomes an essential part to be
considered in the design of the data space. Semantic data models might
also be relevant for optional functions such as billing and auditing.

How best to manage the publication of vocabularies depends on the design
of the data space and its requirements. There can be central servers
hosting the semantic models, public semantic models from industry
associations that can be referenced externally, a group of participants
responsible for publishing and synchronizing common semantic models, or
semantic models that each participant receives when joining the data
space and which can be continuously updated through various
synchronization mechanisms.

![Vocabularies and their relationship to data assets](./media/Vocabularies_and_data_assets.png)

### Optional functions

In addition to the functional elements of a data space, many optional
roles and components exist. The entities providing these functions must
join the data space like any other participant and fulfill all
requirements, policies and procedures enforced by the DSGA to establish
trust.

Depending on the services provided, these additional elements may need
to issue additional credentials, introduce additional trust anchors, or
require specific data contracts. There is a wide variety of optional
roles and services. Some especially useful ones are described here.

In general such optional functions can be distinguished as intermediary
functions or value-creating functions. Intermediaries can participate in
data spaces as value-creating services or functions.

**Intermediaries** are considered as optional in data spaces. Due to certain
regulations like the Data Governance Act, such intermediaries may require
additional governance.

**Value-adding services** may be realized by intermediaries or as function
of a data space participant. Such value-adding services are not subject
to the IDSA Rulebook, but are explained in the [DSSC Blueprint Version 1](https://dssc.eu/)
in more detail. The IDSA Rulebook provides a limited explanation below.

#### Marketplaces

Data sharing always takes place peer-to-peer in a data space with data
discovery being provided via catalogs. This basic functionality does not
cover any form of business model. Since many dataspaces require not only
searching for available data but also platforms for trading, buying, and
selling data, it is expected that many different models of data
marketplaces will emerge within data spaces.

Again, these can be centralized marketplaces, federated marketplaces, or
individual decentralized business platforms. Similar to how resources
can be bought and sold on exchanges, functions can be created for data
contracts. A marketplace can also provide a catalog that enables data
discovery as well as a business platform to buy and sell data. Or it
simply may act as a broker facilitating the negotiation of data
contracts for a fee.

#### Processing services

A data space can have participants that do not offer their data and are
not the end users of data. At its most basic level, these can be
participants that are offering algorithms and code for processing data
as a data contract to deliver code libraries, signed containers, or
entire virtual machines to other participants. For very computation
intensive or special hardware requiring workloads these participants
might offer their own infrastructure as part of the contract and use
policies to control the use of their resources.

Many data spaces can be built on top of the peer-to-peer model, such as
a data supply chain where data assets pass through multiple processors
before reaching the end user. The implementation and capability of these
services again depends on the architecture, policies, and rules of the
data space.

#### Data escrow, data trustee

For many applications, data assets and algorithms from multiple sources
need to be combined to generate value. This will lead to trusted service
providers collecting all necessary data, perform the calculations, and
then distribute the results - while adhering to all contract policies
and guaranteeing the execution of usage policies such as the enforcement
of deletion rules. The business model for these participants will be
only to provide trusted services and not to use the data.

Plenty of possible models are conceivable, from centralized, federated
to decentralized offerings with different technical capabilities, trust
levels and costs. Classic data aggregation platforms such as data lakes
can also be a possible implementation and benefit from the trust which a
data space provides.

## Technical components of a data space

### Data space governance authority services

Several services are required that represent the functional role of the
data space governance authority (DSGA) to enable the management functions of a data
space. These services may be designed as centralized, federated
(distributed) or decentralized services (See below for more information
on the differences between these solution designs). Depending on which
design is chosen, these services can be implemented with varying
component designs that best support the needs of the data space.

Regardless of the technical implementation and the specific architecture
model, the following components are required:

- Registration: A service providing the requirements of the data space
    to apply for membership (includes the validation of attributes and
    their values of the participant self-description and checking their
    applicability against membership policies). This service can be
    machine based but can also include human workflows.

- Membership credentials: a membership issuance and verification
    service can be used to manage membership credentials. Also
    responsible for revocation of credentials.

- Participant directory: Enables the discovery of other participants
    in the data space.

### Identity

The design of the identity provider is the first decision for the design
of the data space. If a central identity provider is chosen to manage
the identities for all participants, every other service depends on this
central verification, and decentralized designs are no longer fully
feasible.

Which mechanism to use to identify participants is the most fundamental
design decision. It impacts policies on autonomy and sovereignty as well
as technical solution architectures for other components of a data
space.

| **Identity System**       | **Advantages**            | **Disadvantages**        |
|--- |--- | --- |
| **Centralized identity**  | Simple management for DSGA | Low autonomy and sovereignty of participants |
|  | High degree of control for DSGA    | Single point of failure |
|  | Traditional, well-known technology stack | Single point of attack |
|  |  | Harder to manage for participants |
| **Decentralized  identities**       | Full autonomy and overeignty for participants | Complexity: DSGA management requires decentralized protocols |
|  | Low resourcing need for DSGA | Lower degree of control for DSGA |
|  | Easy to manage for participants | New and partially unfamiliar technology stack|
|  | Harder to attack |  |

### Catalog

The catalog component supports the search for available data contracts.
Information about data contracts can be exchanged between participants
without the use of a catalog by sending the offer directly via a
separate channel (e-mail, notification). A catalog will be a common
component to implement data discoverability. It can be implemented as a
managed service by one or more selected participants, hosted by the data
space governance authority, or operated in a fully decentralized fashion by every
participant that offers data contracts (see the visual representation of
various implementation designs of the DSGA above). The type of catalog
architecture used depends on the design of the data space as well as the
needs and capabilities of the participants.

Hybrid catalog models combining central and distributed catalogs with
individual decentralized catalogs are possible, but must be carefully
designed to avoid unnecessarily increasing the complexity of
participating in the data space.

#### Attributes & self-description

Attributes and self-description should always be available as verified
presentations. The exact serialization format and service endpoints
depend on the implementation of the data space and the trust anchors in
use.

### Connector

The connector forms the gateway for a participant to a data space. It
provides the necessary API endpoints for other participants to negotiate
data contracts and request the execution of a data contract. The
connector acts as an agent of the participant to the data space.

Which solution components are provided by the connector beyond the
contract negotiation and execution depends on the implementation design
of the data space.

### Observer

As described above, there is no specific technical component for an
observer as this is a role within the data space and not a component.

### Vocabulary

The semantic model for the policies and self-descriptions required to
join the data space is provided by the DSGA. It may also provide semantic
models that need to be understood throughout the data space and might be
mandatory for the publication and use of specific data contracts.

The DSGA must decide how semantic models are provided, whether by
reference to a known, standardized schema externally or through a
vocabulary service provided by the DSGA or specific participants.

Individual participants may provide additional vocabulary services to
enable the discovery of semantic models needed to successfully share
data with that participant. These could be additional semantic policies
or semantic models that describe the shared data model. For example, the
semantic model of the shared data must be understood by the consumer to
properly manage consent for GDPR.

As mentioned before, the importance of the implementation design of the
DSGA and the components of a data space cannot be emphasized enough. The
implications for autonomy, sovereignty, reliability, security, and many
other factors are far reaching, so the decision on the design needs to
be made with utmost care.

### "Central," or "federated/distributed," or "decentralized"

![Variants for Data Space Governance Authorities](media/Variants_for_dsga.png)

#### Centralized data space governance authority

In a centralized DSGA design, the entity runs all services to operate the
data space. These include services to identify participants, onboard new
participants, manage memberships, provide semantic models, discover data
and optional services like marketplaces and audits.

While this model is popular due to the familiarity with centralized
models through existing aggregator platforms, it limits the autonomy and
sovereignty of participants. If a centralized identity provider is used,
the entity that controls the identity provider also controls membership
and access to resources. This entity could make arbitrary decisions on
inclusion or exclusion without regard to the policies of the data space.
Worst case, such a central identity service could interfere with the
data sharing between two participants, with serious consequences beyond
the data space.

A central catalog has advantages for data discovery as it provides a
known location to discover available data and queries only need to be
made at one endpoint and data contract offers are returned from multiple
participants. But it poses the risk that the entity controlling the
catalog also controls its content and make arbitrary decisions which
items are available to whom.

Centralized services also create a single point of failure. Outage could
result in the entire data space becoming unavailable or inoperable. This
could cause a significant business risk for participants.

If the data shared is valuable data that should be highly protected, it
could attract bad actors trying to gain access, manipulate it or simply
disrupt operations to harm their targets. When a lot of value is
aggregated into a centralized component, it could become the target. An
infiltrated central identity provider or catalog could create more
damage than if a single participant is attacked.

With careful planning and the right choices when implementing a
centralized data space, many of the issues that can prevent participant
autonomy can be avoided or softened. But vital functional resources of
the data space do not allow for full autonomy of participants in this
design solution. However, depending on the purpose and goals of the data
space this may not be a problem.

#### Federated / distributed data space governance authority

The federated or distributed model retains some degree of centralized
control but improves on the technical and security challenges. In this
model, functional roles are distributed to a few federated nodes.
Instead of just one entity providing a service, multiple entities share
responsibility for providing this service through individual nodes that
are synchronized. This requires some additional technical investment as
nodes need to be synchronized, transactions handled, and queries
performed across multiple services.

While this model strongly improves resilience and availability, it also
increases complexity. Some functional roles are more complex to
implement in a distributed environment (e.g., identity) than others
(e.g., catalog). However, it offers interesting variations on the
centralized design by allowing more sophisticated designs. For example,
a federated catalog could be implemented so that different sub-catalogs
are available on different nodes, instead of synchronizing all entries
everywhere, increasing performance and availability of the system.

If the goal of the data space is to maximize participant sovereignty and
autonomy, the distributed model does not provide significant
improvements in comparison to the centralized design because a small
group of entities would have most control over the data space and the
participants would be almost as dependent on these entities as in a
centralized data space.

Nevertheless, a federated model can be the optimal solution to implement
data spaces based on closed group consortia with clear consortia
leaders. There may be reasons beyond the technical design, such as
contracts and legal regulations that necessitate implementing a data
space as a federated or partially federated model.

When talking about distributed data spaces there is a distinction
between "*Federation service"* and *"Federated service".*

- Federation service supports the federation functionality of a data
    space and serves a functional role such as identity or catalog.

- Federated service describes the implementation of any service as a
    distributed service in a data space, including but not limited to
    any of the federation services.

To maximize the sovereignty and autonomy of participants in a data
space, every participant must be free to act without being improperly
impeded by anybody. A participant must follow the rules and adhere to
policies, but a sovereign participant needs to be immune from undue or
random interference. Improper interference can include refusal to put a
participant's data assets in the catalog despite meeting all
requirements or deactivating the participants identity and thus
potentially disrupting the participant's business. This may not be
malicious interference; errors can happen, and the software could be
unstable. A fully sovereign participant must be able to interact with
other participants without depending on a third party once it is proven
that the participant is following all rules.

#### Decentralized data space governance authority

Using a decentralized design enables the highest level of autonomy and
sovereignty. The core element enabling a participant to act autonomously
is the identity system. By using a decentralized identity system each
participant is responsible to maintain identity information that can be
verified by other participants or the DSGA, rather than relying on a
centralized identity provider.

Once decentralized identities are established, all other functional
services can also to be decentralized, minimizing or even eliminating
barriers to participant sovereignty.

It should be noted that in a decentralized data space a lot of the
responsibility for operating essential functional roles shifts from the
DSGA to the participants. For example, in a centralized model, the DSGA is
expected to operate the catalog of available data assets, while in a
decentralized model, each participant is responsible for publishing its
available data directly and in turn, each participant needs to ask all
other participants about their available assets.

Another advantage of a decentralized system is that it is usually more
resilient to errors or bad actors, since problems in individual nodes do
not automatically affect all participants of the data space. Finally, a
decentralized system does not require an ever-increasing number of
centralized services. Each node is self-contained and provides all the
endpoints necessary to interact with it. A data space can grow and scale
much more efficiently than a centralized design, where the resources to
provide central services must grow exponentially.

### Decision areas

#### Sovereignty

The goal of digital sovereignty is autonomy, which is different from
independence -- it means acting with choice. It includes control over
when and where data is stored and how it can be accessed. Sovereignty
and autonomy are not binary concepts but move along a spectrum. The goal
is to increase sovereignty and autonomy until a desired threshold is
reached. In that sense, the concept is similar to that of privacy.

#### Resilience

Resilience in a data space is about the ability of the ecosystem and
individual actors to continue functioning in the event of unforeseen
problems.

#### Scalability

Scalability of a data space is not about the volume of data but about
the number of participants, the amount of the data assets shared, and
the number of negotiated contracts.

#### Control

In this context, a high level of control means that the entity operating
the DSGA can control access to the services as well as the content they
provide. This is in direct contrast to sovereignty, where the control
lies with the individual participant.

#### Simplicity

Well-established technologies and architecture models are easier to
deploy because implementing teams have experience with them. The
interaction model between participants as well as the business model of
the data space are included in this category.

#### Discoverability

Discoverability is the measure of how many steps are necessary to find
the data offered in the data space. Since data asset information can
always be exchanged directly between participants, this measure only
considers how complex a query would be to find all data assets currently
offered in the data space.

### Decision support

As all decision areas are connected and partially work against each
other, it is necessary to look at them holistically and not focus on one
area. Make sure you weigh the importance of these decisions according to
your business and technical needs. The technical maturity of the planned
participants is an important factor. Many organizations are willing to
compromise on their digital sovereignty in exchange for convenience and
business value.

Many models exist in between the main three implementation designs. The
following charts highlight some of the interdependencies between the
decision areas for planning, implementing and operating a data space:

With a centralized design the entity operating identity and catalog
services has a lot of control. It is easy to setup, only one entity
needs to deal with the DSGA services, and participants can simply query
one catalog and rely on the DSGA as a trust anchor to issue a participant
ID. But this design impairs participant sovereignty, is less resilient
and difficult to scale as the central services will grow exponentially
in their resource requirements as more participants join.

The distributed design sits in the middle of the spectrum. Control is
not exercised by a single entity but by multiple federators and thus not
a single entity can make arbitrary decisions. However, participants
still do not have full control over their actions, so sovereignty is
still impaired. Resilience and scalability are improved by having
multiple nodes of the data space services that can either be setup as
partitions or as replicas. Discoverability must take into account the
partitioning of the catalog and might become more complex.

The aim of the decentralized design is to maximize the sovereignty of
individual participants and grant them as much autonomy as possible.
This reduction in dependency on central services automatically leads to
higher resilience and better scalability. However, it adds complexity
for the individual participant, as all participants now need to operate
service nodes that participate in the discovery process of available
data. Some data spaces might require additional control over
participants and their actions, which is harder to achieve in a
decentralized implementation.

The figure below gives a comprehensive overview of the values within the
decision areas when implementing a centralized, federated/distributed,
or decentralized approach.

![Comparison of models for decision support](media/media/image16.png)

Another way to compare the features and
capabilities of the different designs is to separate the decision areas
into a business and a technical perspective. Which design benefits the
business value of the data space vs. which design aspects are a
technical necessity? A careful compromise design-decision can be voted
on by the founding parties of the data space to reach the optimal
implementation.

These three models are just examples of possible implementation designs.
Every data space should be tailored to the needs of its participants.
Any entity that wishes to participate in a data space should investigate
the implementation design in detail to ensure the design grants them the
aspired level of sovereignty and supports its business goals.
